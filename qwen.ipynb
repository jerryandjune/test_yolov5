{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c74430d-4039-46c4-89a7-a294e0a6cf25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple/\n",
      "Requirement already satisfied: transformers<4.38.0,>=4.32.0 in /root/miniconda3/lib/python3.10/site-packages (from -r Qwen-main/requirements.txt (line 1)) (4.37.2)\n",
      "Requirement already satisfied: accelerate in /root/miniconda3/lib/python3.10/site-packages (from -r Qwen-main/requirements.txt (line 2)) (0.33.0)\n",
      "Requirement already satisfied: tiktoken in /root/miniconda3/lib/python3.10/site-packages (from -r Qwen-main/requirements.txt (line 3)) (0.7.0)\n",
      "Requirement already satisfied: einops in /root/miniconda3/lib/python3.10/site-packages (from -r Qwen-main/requirements.txt (line 4)) (0.8.0)\n",
      "Requirement already satisfied: transformers_stream_generator==0.0.4 in /root/miniconda3/lib/python3.10/site-packages (from -r Qwen-main/requirements.txt (line 5)) (0.0.4)\n",
      "Requirement already satisfied: scipy in /root/miniconda3/lib/python3.10/site-packages (from -r Qwen-main/requirements.txt (line 6)) (1.14.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /root/miniconda3/lib/python3.10/site-packages (from transformers<4.38.0,>=4.32.0->-r Qwen-main/requirements.txt (line 1)) (0.24.5)\n",
      "Requirement already satisfied: numpy>=1.17 in /root/miniconda3/lib/python3.10/site-packages (from transformers<4.38.0,>=4.32.0->-r Qwen-main/requirements.txt (line 1)) (1.26.4)\n",
      "Requirement already satisfied: requests in /root/miniconda3/lib/python3.10/site-packages (from transformers<4.38.0,>=4.32.0->-r Qwen-main/requirements.txt (line 1)) (2.32.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /root/miniconda3/lib/python3.10/site-packages (from transformers<4.38.0,>=4.32.0->-r Qwen-main/requirements.txt (line 1)) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /root/miniconda3/lib/python3.10/site-packages (from transformers<4.38.0,>=4.32.0->-r Qwen-main/requirements.txt (line 1)) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /root/miniconda3/lib/python3.10/site-packages (from transformers<4.38.0,>=4.32.0->-r Qwen-main/requirements.txt (line 1)) (2024.7.24)\n",
      "Requirement already satisfied: tqdm>=4.27 in /root/miniconda3/lib/python3.10/site-packages (from transformers<4.38.0,>=4.32.0->-r Qwen-main/requirements.txt (line 1)) (4.66.4)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /root/miniconda3/lib/python3.10/site-packages (from transformers<4.38.0,>=4.32.0->-r Qwen-main/requirements.txt (line 1)) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /root/miniconda3/lib/python3.10/site-packages (from transformers<4.38.0,>=4.32.0->-r Qwen-main/requirements.txt (line 1)) (0.4.3)\n",
      "Requirement already satisfied: filelock in /root/miniconda3/lib/python3.10/site-packages (from transformers<4.38.0,>=4.32.0->-r Qwen-main/requirements.txt (line 1)) (3.14.0)\n",
      "Requirement already satisfied: torch>=1.10.0 in /root/miniconda3/lib/python3.10/site-packages (from accelerate->-r Qwen-main/requirements.txt (line 2)) (2.1.2+cu118)\n",
      "Requirement already satisfied: psutil in /root/miniconda3/lib/python3.10/site-packages (from accelerate->-r Qwen-main/requirements.txt (line 2)) (5.9.8)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /root/miniconda3/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers<4.38.0,>=4.32.0->-r Qwen-main/requirements.txt (line 1)) (4.12.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /root/miniconda3/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers<4.38.0,>=4.32.0->-r Qwen-main/requirements.txt (line 1)) (2024.5.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /root/miniconda3/lib/python3.10/site-packages (from requests->transformers<4.38.0,>=4.32.0->-r Qwen-main/requirements.txt (line 1)) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /root/miniconda3/lib/python3.10/site-packages (from requests->transformers<4.38.0,>=4.32.0->-r Qwen-main/requirements.txt (line 1)) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /root/miniconda3/lib/python3.10/site-packages (from requests->transformers<4.38.0,>=4.32.0->-r Qwen-main/requirements.txt (line 1)) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /root/miniconda3/lib/python3.10/site-packages (from requests->transformers<4.38.0,>=4.32.0->-r Qwen-main/requirements.txt (line 1)) (2.0.4)\n",
      "Requirement already satisfied: jinja2 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.10.0->accelerate->-r Qwen-main/requirements.txt (line 2)) (3.1.4)\n",
      "Requirement already satisfied: networkx in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.10.0->accelerate->-r Qwen-main/requirements.txt (line 2)) (3.3)\n",
      "Requirement already satisfied: sympy in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.10.0->accelerate->-r Qwen-main/requirements.txt (line 2)) (1.12.1)\n",
      "Requirement already satisfied: triton==2.1.0 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.10.0->accelerate->-r Qwen-main/requirements.txt (line 2)) (2.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /root/miniconda3/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate->-r Qwen-main/requirements.txt (line 2)) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /root/miniconda3/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate->-r Qwen-main/requirements.txt (line 2)) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -r Qwen-main/requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "514eaa0a-6d7f-4e0c-a99b-dc9e4d784dea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-08-02 20:26:01--  https://www.modelscope.cn/models/qwen/Qwen-7B-Chat-Int4/resolve/master/model-00001-of-00003.safetensors\n",
      "Resolving www.modelscope.cn (www.modelscope.cn)... 39.99.230.9, 39.99.245.74, 39.101.133.162, ...\n",
      "Connecting to www.modelscope.cn (www.modelscope.cn)|39.99.230.9|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2040563808 (1.9G) [application/octet-stream]\n",
      "Saving to: ‘model-00001-of-00003.safetensors’\n",
      "\n",
      "model-00001-of-0000 100%[===================>]   1.90G  23.5MB/s    in 88s     \n",
      "\n",
      "2024-08-02 20:27:29 (22.1 MB/s) - ‘model-00001-of-00003.safetensors’ saved [2040563808/2040563808]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://www.modelscope.cn/models/qwen/Qwen-7B-Chat-Int4/resolve/master/model-00001-of-00003.safetensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23ae8878-7055-4aa0-8cb4-8da5724abc92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-08-02 20:27:43--  https://www.modelscope.cn/models/qwen/Qwen-7B-Chat-Int4/resolve/master/model-00002-of-00003.safetensors\n",
      "Resolving www.modelscope.cn (www.modelscope.cn)... 39.99.250.5, 39.98.80.214, 39.99.230.9, ...\n",
      "Connecting to www.modelscope.cn (www.modelscope.cn)|39.99.250.5|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2047911536 (1.9G) [application/octet-stream]\n",
      "Saving to: ‘model-00002-of-00003.safetensors’\n",
      "\n",
      "model-00002-of-0000 100%[===================>]   1.91G  21.8MB/s    in 90s     \n",
      "\n",
      "2024-08-02 20:29:13 (21.8 MB/s) - ‘model-00002-of-00003.safetensors’ saved [2047911536/2047911536]\n",
      "\n",
      "--2024-08-02 20:29:14--  https://www.modelscope.cn/models/qwen/Qwen-7B-Chat-Int4/resolve/master/model-00003-of-00003.safetensors\n",
      "Resolving www.modelscope.cn (www.modelscope.cn)... 39.99.230.9, 39.99.245.74, 39.100.73.80, ...\n",
      "Connecting to www.modelscope.cn (www.modelscope.cn)|39.99.230.9|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1772181480 (1.7G) [application/octet-stream]\n",
      "Saving to: ‘model-00003-of-00003.safetensors’\n",
      "\n",
      "model-00003-of-0000 100%[===================>]   1.65G  17.3MB/s    in 87s     \n",
      "\n",
      "2024-08-02 20:30:41 (19.4 MB/s) - ‘model-00003-of-00003.safetensors’ saved [1772181480/1772181480]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://www.modelscope.cn/models/qwen/Qwen-7B-Chat-Int4/resolve/master/model-00002-of-00003.safetensors\n",
    "!wget https://www.modelscope.cn/models/qwen/Qwen-7B-Chat-Int4/resolve/master/model-00003-of-00003.safetensors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "00587d2c-1885-41f6-8c6d-958a3da22986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple/\n",
      "Requirement already satisfied: deepspeed in /root/miniconda3/lib/python3.10/site-packages (0.14.4)\n",
      "Requirement already satisfied: peft in /root/miniconda3/lib/python3.10/site-packages (0.12.0)\n",
      "Requirement already satisfied: auto-gptq in /root/miniconda3/lib/python3.10/site-packages (0.7.1)\n",
      "Requirement already satisfied: optimum in /root/miniconda3/lib/python3.10/site-packages (1.21.2)\n",
      "Requirement already satisfied: psutil in /root/miniconda3/lib/python3.10/site-packages (from deepspeed) (5.9.8)\n",
      "Requirement already satisfied: py-cpuinfo in /root/miniconda3/lib/python3.10/site-packages (from deepspeed) (9.0.0)\n",
      "Requirement already satisfied: numpy in /root/miniconda3/lib/python3.10/site-packages (from deepspeed) (1.26.4)\n",
      "Requirement already satisfied: hjson in /root/miniconda3/lib/python3.10/site-packages (from deepspeed) (3.1.0)\n",
      "Requirement already satisfied: nvidia-ml-py in /root/miniconda3/lib/python3.10/site-packages (from deepspeed) (12.555.43)\n",
      "Requirement already satisfied: pydantic in /root/miniconda3/lib/python3.10/site-packages (from deepspeed) (2.8.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /root/miniconda3/lib/python3.10/site-packages (from deepspeed) (24.1)\n",
      "Requirement already satisfied: torch in /root/miniconda3/lib/python3.10/site-packages (from deepspeed) (2.1.2+cu118)\n",
      "Requirement already satisfied: tqdm in /root/miniconda3/lib/python3.10/site-packages (from deepspeed) (4.66.4)\n",
      "Requirement already satisfied: ninja in /root/miniconda3/lib/python3.10/site-packages (from deepspeed) (1.11.1.1)\n",
      "Requirement already satisfied: safetensors in /root/miniconda3/lib/python3.10/site-packages (from peft) (0.4.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.17.0 in /root/miniconda3/lib/python3.10/site-packages (from peft) (0.24.5)\n",
      "Requirement already satisfied: transformers in /root/miniconda3/lib/python3.10/site-packages (from peft) (4.37.2)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /root/miniconda3/lib/python3.10/site-packages (from peft) (0.33.0)\n",
      "Requirement already satisfied: pyyaml in /root/miniconda3/lib/python3.10/site-packages (from peft) (6.0.1)\n",
      "Requirement already satisfied: gekko in /root/miniconda3/lib/python3.10/site-packages (from auto-gptq) (1.2.1)\n",
      "Requirement already satisfied: sentencepiece in /root/miniconda3/lib/python3.10/site-packages (from auto-gptq) (0.2.0)\n",
      "Requirement already satisfied: rouge in /root/miniconda3/lib/python3.10/site-packages (from auto-gptq) (1.0.1)\n",
      "Requirement already satisfied: datasets in /root/miniconda3/lib/python3.10/site-packages (from auto-gptq) (2.20.0)\n",
      "Requirement already satisfied: sympy in /root/miniconda3/lib/python3.10/site-packages (from optimum) (1.12.1)\n",
      "Requirement already satisfied: coloredlogs in /root/miniconda3/lib/python3.10/site-packages (from optimum) (15.0.1)\n",
      "Requirement already satisfied: filelock in /root/miniconda3/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (3.14.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /root/miniconda3/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (4.12.2)\n",
      "Requirement already satisfied: requests in /root/miniconda3/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2.32.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /root/miniconda3/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2024.5.0)\n",
      "Requirement already satisfied: jinja2 in /root/miniconda3/lib/python3.10/site-packages (from torch->deepspeed) (3.1.4)\n",
      "Requirement already satisfied: networkx in /root/miniconda3/lib/python3.10/site-packages (from torch->deepspeed) (3.3)\n",
      "Requirement already satisfied: triton==2.1.0 in /root/miniconda3/lib/python3.10/site-packages (from torch->deepspeed) (2.1.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /root/miniconda3/lib/python3.10/site-packages (from transformers->peft) (0.15.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /root/miniconda3/lib/python3.10/site-packages (from transformers->peft) (2024.7.24)\n",
      "Requirement already satisfied: protobuf in /root/miniconda3/lib/python3.10/site-packages (from transformers->peft) (4.25.3)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /root/miniconda3/lib/python3.10/site-packages (from coloredlogs->optimum) (10.0)\n",
      "Requirement already satisfied: xxhash in /root/miniconda3/lib/python3.10/site-packages (from datasets->auto-gptq) (3.4.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /root/miniconda3/lib/python3.10/site-packages (from datasets->auto-gptq) (17.0.0)\n",
      "Requirement already satisfied: pandas in /root/miniconda3/lib/python3.10/site-packages (from datasets->auto-gptq) (2.2.2)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /root/miniconda3/lib/python3.10/site-packages (from datasets->auto-gptq) (0.3.8)\n",
      "Requirement already satisfied: multiprocess in /root/miniconda3/lib/python3.10/site-packages (from datasets->auto-gptq) (0.70.16)\n",
      "Requirement already satisfied: pyarrow-hotfix in /root/miniconda3/lib/python3.10/site-packages (from datasets->auto-gptq) (0.6)\n",
      "Requirement already satisfied: aiohttp in /root/miniconda3/lib/python3.10/site-packages (from datasets->auto-gptq) (3.10.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /root/miniconda3/lib/python3.10/site-packages (from pydantic->deepspeed) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /root/miniconda3/lib/python3.10/site-packages (from pydantic->deepspeed) (2.20.1)\n",
      "Requirement already satisfied: six in /root/miniconda3/lib/python3.10/site-packages (from rouge->auto-gptq) (1.16.0)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /root/miniconda3/lib/python3.10/site-packages (from sympy->optimum) (1.3.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets->auto-gptq) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets->auto-gptq) (4.0.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets->auto-gptq) (2.3.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets->auto-gptq) (1.9.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets->auto-gptq) (6.0.5)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets->auto-gptq) (1.4.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets->auto-gptq) (23.2.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /root/miniconda3/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2.2.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /root/miniconda3/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /root/miniconda3/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /root/miniconda3/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.7.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /root/miniconda3/lib/python3.10/site-packages (from jinja2->torch->deepspeed) (2.1.5)\n",
      "Requirement already satisfied: pytz>=2020.1 in /root/miniconda3/lib/python3.10/site-packages (from pandas->datasets->auto-gptq) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /root/miniconda3/lib/python3.10/site-packages (from pandas->datasets->auto-gptq) (2024.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /root/miniconda3/lib/python3.10/site-packages (from pandas->datasets->auto-gptq) (2.9.0.post0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install deepspeed peft auto-gptq optimum -i https://pypi.tuna.tsinghua.edu.cn/simple/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "71543d4e-17e1-4504-a140-3ee6cbec8149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/autodl-tmp/Qwen-main\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d7beca76-17e8-4745-b95c-6cd4caaa912d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/autodl-tmp/Qwen-main\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd /root/autodl-tmp/Qwen-main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "40c9e5e1-50b6-4df9-aa31-bd4a5d640551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 22.11.1\n",
      "  latest version: 24.7.1\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "Or to minimize the number of packages updated during conda update use\n",
      "\n",
      "     conda install conda=24.7.1\n",
      "\n",
      "\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /root/miniconda3\n",
      "\n",
      "  added / updated specs:\n",
      "    - mpi4py\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    ca-certificates-2023.08.22 |       h06a4308_0         130 KB  anaconda\n",
      "    certifi-2023.11.17         |  py310h06a4308_0         159 KB  anaconda\n",
      "    libgfortran-ng-7.5.0       |      ha8ba4b0_17          22 KB  anaconda\n",
      "    libgfortran4-7.5.0         |      ha8ba4b0_17         1.3 MB  anaconda\n",
      "    mpi-1.0                    |            mpich          13 KB  anaconda\n",
      "    mpi4py-3.1.4               |  py310hfc96bbd_0         606 KB  anaconda\n",
      "    mpich-3.3.2                |       hc856adb_0         6.4 MB  anaconda\n",
      "    openssl-1.1.1w             |       h7f8727e_0         3.8 MB  anaconda\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:        12.4 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  libgfortran-ng     anaconda/linux-64::libgfortran-ng-7.5.0-ha8ba4b0_17 \n",
      "  libgfortran4       anaconda/linux-64::libgfortran4-7.5.0-ha8ba4b0_17 \n",
      "  mpi                anaconda/linux-64::mpi-1.0-mpich \n",
      "  mpi4py             anaconda/linux-64::mpi4py-3.1.4-py310hfc96bbd_0 \n",
      "  mpich              anaconda/linux-64::mpich-3.3.2-hc856adb_0 \n",
      "\n",
      "The following packages will be SUPERSEDED by a higher-priority channel:\n",
      "\n",
      "  ca-certificates    anaconda/pkgs/main::ca-certificates-2~ --> anaconda::ca-certificates-2023.08.22-h06a4308_0 \n",
      "  certifi            anaconda/pkgs/main::certifi-2024.7.4-~ --> anaconda::certifi-2023.11.17-py310h06a4308_0 \n",
      "  openssl                                anaconda/pkgs/main --> anaconda \n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "openssl-1.1.1w       | 3.8 MB    |                                       |   0% \n",
      "ca-certificates-2023 | 130 KB    |                                       |   0% \u001b[A\n",
      "\n",
      "certifi-2023.11.17   | 159 KB    |                                       |   0% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "mpi-1.0              | 13 KB     |                                       |   0% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "libgfortran-ng-7.5.0 | 22 KB     |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "mpi4py-3.1.4         | 606 KB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libgfortran4-7.5.0   | 1.3 MB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "mpich-3.3.2          | 6.4 MB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "openssl-1.1.1w       | 3.8 MB    | 1                                     |   0% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "libgfortran-ng-7.5.0 | 22 KB     | ###########################2          |  74% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "ca-certificates-2023 | 130 KB    | ####5                                 |  12% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "openssl-1.1.1w       | 3.8 MB    | 6                                     |   2% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "mpi-1.0              | 13 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "certifi-2023.11.17   | 159 KB    | ##############8                       |  40% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "mpi-1.0              | 13 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\n",
      "ca-certificates-2023 | 130 KB    | ##################2                   |  49% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "mpi4py-3.1.4         | 606 KB    | 9                                     |   3% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "openssl-1.1.1w       | 3.8 MB    | #2                                    |   3% \u001b[A\n",
      "\n",
      "certifi-2023.11.17   | 159 KB    | #############################7        |  80% \u001b[A\u001b[A\n",
      "ca-certificates-2023 | 130 KB    | ##################################### | 100% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libgfortran4-7.5.0   | 1.3 MB    | 4                                     |   1% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "certifi-2023.11.17   | 159 KB    | ##################################### | 100% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "mpi4py-3.1.4         | 606 KB    | ###9                                  |  11% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "openssl-1.1.1w       | 3.8 MB    | ###1                                  |   9% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "mpich-3.3.2          | 6.4 MB    | 9                                     |   2% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libgfortran4-7.5.0   | 1.3 MB    | #8                                    |   5% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "openssl-1.1.1w       | 3.8 MB    | ######                                |  16% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "openssl-1.1.1w       | 3.8 MB    | #########                             |  25% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "mpi4py-3.1.4         | 606 KB    | #########################3            |  69% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libgfortran4-7.5.0   | 1.3 MB    | ###2                                  |   9% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "openssl-1.1.1w       | 3.8 MB    | ############5                         |  34% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libgfortran4-7.5.0   | 1.3 MB    | ####6                                 |  12% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "openssl-1.1.1w       | 3.8 MB    | ##############8                       |  40% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "mpi4py-3.1.4         | 606 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "mpi4py-3.1.4         | 606 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "mpich-3.3.2          | 6.4 MB    | #################                     |  46% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "openssl-1.1.1w       | 3.8 MB    | ##############################7       |  83% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "mpich-3.3.2          | 6.4 MB    | ######################5               |  61% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libgfortran4-7.5.0   | 1.3 MB    | ################1                     |  44% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "openssl-1.1.1w       | 3.8 MB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "mpich-3.3.2          | 6.4 MB    | ###############################6      |  86% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libgfortran4-7.5.0   | 1.3 MB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libgfortran4-7.5.0   | 1.3 MB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "mpich-3.3.2          | 6.4 MB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n"
     ]
    }
   ],
   "source": [
    "!conda install -c anaconda mpi4py -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "22aa1b1a-13c4-48e4-931c-ea5c6a9c9594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-08-02 21:07:09,103] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
      "\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-dev package with apt\n",
      "\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
      "\u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n",
      "\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1\n",
      "\u001b[93m [WARNING] \u001b[0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible\n",
      "/root/miniconda3/lib/python3.10/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
      "  warnings.warn(\n",
      "Using `disable_exllama` is deprecated and will be removed in version 4.37. Use `use_exllama` instead and specify the version with `exllama_config`.The value of `use_exllama` will be overwritten by `disable_exllama` passed in `GPTQConfig` or stored in your config file.\n",
      "You passed `quantization_config` to `from_pretrained` but the model you're loading already has a `quantization_config` attribute and has already quantized weights. However, loading attributes (e.g. ['use_cuda_fp16', 'use_exllama', 'max_input_length', 'exllama_config', 'disable_exllama']) will be overwritten with the one you passed to `from_pretrained`. The rest will be ignored.\n",
      "CUDA extension not installed.\n",
      "CUDA extension not installed.\n",
      "Try importing flash-attention for faster inference...\n",
      "Warning: import flash_attn rotary fail, please install FlashAttention rotary to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/rotary\n",
      "Warning: import flash_attn rms_norm fail, please install FlashAttention layer_norm to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/layer_norm\n",
      "Warning: import flash_attn fail, please install FlashAttention to get higher efficiency https://github.com/Dao-AILab/flash-attention\n",
      "Loading checkpoint shards: 100%|██████████████████| 3/3 [00:02<00:00,  1.14it/s]\n",
      "Some weights of the model checkpoint at Qwen-7B-Chat-Int4 were not used when initializing QWenLMHeadModel: ['transformer.h.0.attn.c_proj.bias', 'transformer.h.0.mlp.c_proj.bias', 'transformer.h.0.mlp.w1.bias', 'transformer.h.0.mlp.w2.bias', 'transformer.h.1.attn.c_proj.bias', 'transformer.h.1.mlp.c_proj.bias', 'transformer.h.1.mlp.w1.bias', 'transformer.h.1.mlp.w2.bias', 'transformer.h.10.attn.c_proj.bias', 'transformer.h.10.mlp.c_proj.bias', 'transformer.h.10.mlp.w1.bias', 'transformer.h.10.mlp.w2.bias', 'transformer.h.11.attn.c_proj.bias', 'transformer.h.11.mlp.c_proj.bias', 'transformer.h.11.mlp.w1.bias', 'transformer.h.11.mlp.w2.bias', 'transformer.h.12.attn.c_proj.bias', 'transformer.h.12.mlp.c_proj.bias', 'transformer.h.12.mlp.w1.bias', 'transformer.h.12.mlp.w2.bias', 'transformer.h.13.attn.c_proj.bias', 'transformer.h.13.mlp.c_proj.bias', 'transformer.h.13.mlp.w1.bias', 'transformer.h.13.mlp.w2.bias', 'transformer.h.14.attn.c_proj.bias', 'transformer.h.14.mlp.c_proj.bias', 'transformer.h.14.mlp.w1.bias', 'transformer.h.14.mlp.w2.bias', 'transformer.h.15.attn.c_proj.bias', 'transformer.h.15.mlp.c_proj.bias', 'transformer.h.15.mlp.w1.bias', 'transformer.h.15.mlp.w2.bias', 'transformer.h.16.attn.c_proj.bias', 'transformer.h.16.mlp.c_proj.bias', 'transformer.h.16.mlp.w1.bias', 'transformer.h.16.mlp.w2.bias', 'transformer.h.17.attn.c_proj.bias', 'transformer.h.17.mlp.c_proj.bias', 'transformer.h.17.mlp.w1.bias', 'transformer.h.17.mlp.w2.bias', 'transformer.h.18.attn.c_proj.bias', 'transformer.h.18.mlp.c_proj.bias', 'transformer.h.18.mlp.w1.bias', 'transformer.h.18.mlp.w2.bias', 'transformer.h.19.attn.c_proj.bias', 'transformer.h.19.mlp.c_proj.bias', 'transformer.h.19.mlp.w1.bias', 'transformer.h.19.mlp.w2.bias', 'transformer.h.2.attn.c_proj.bias', 'transformer.h.2.mlp.c_proj.bias', 'transformer.h.2.mlp.w1.bias', 'transformer.h.2.mlp.w2.bias', 'transformer.h.20.attn.c_proj.bias', 'transformer.h.20.mlp.c_proj.bias', 'transformer.h.20.mlp.w1.bias', 'transformer.h.20.mlp.w2.bias', 'transformer.h.21.attn.c_proj.bias', 'transformer.h.21.mlp.c_proj.bias', 'transformer.h.21.mlp.w1.bias', 'transformer.h.21.mlp.w2.bias', 'transformer.h.22.attn.c_proj.bias', 'transformer.h.22.mlp.c_proj.bias', 'transformer.h.22.mlp.w1.bias', 'transformer.h.22.mlp.w2.bias', 'transformer.h.23.attn.c_proj.bias', 'transformer.h.23.mlp.c_proj.bias', 'transformer.h.23.mlp.w1.bias', 'transformer.h.23.mlp.w2.bias', 'transformer.h.24.attn.c_proj.bias', 'transformer.h.24.mlp.c_proj.bias', 'transformer.h.24.mlp.w1.bias', 'transformer.h.24.mlp.w2.bias', 'transformer.h.25.attn.c_proj.bias', 'transformer.h.25.mlp.c_proj.bias', 'transformer.h.25.mlp.w1.bias', 'transformer.h.25.mlp.w2.bias', 'transformer.h.26.attn.c_proj.bias', 'transformer.h.26.mlp.c_proj.bias', 'transformer.h.26.mlp.w1.bias', 'transformer.h.26.mlp.w2.bias', 'transformer.h.27.attn.c_proj.bias', 'transformer.h.27.mlp.c_proj.bias', 'transformer.h.27.mlp.w1.bias', 'transformer.h.27.mlp.w2.bias', 'transformer.h.28.attn.c_proj.bias', 'transformer.h.28.mlp.c_proj.bias', 'transformer.h.28.mlp.w1.bias', 'transformer.h.28.mlp.w2.bias', 'transformer.h.29.attn.c_proj.bias', 'transformer.h.29.mlp.c_proj.bias', 'transformer.h.29.mlp.w1.bias', 'transformer.h.29.mlp.w2.bias', 'transformer.h.3.attn.c_proj.bias', 'transformer.h.3.mlp.c_proj.bias', 'transformer.h.3.mlp.w1.bias', 'transformer.h.3.mlp.w2.bias', 'transformer.h.30.attn.c_proj.bias', 'transformer.h.30.mlp.c_proj.bias', 'transformer.h.30.mlp.w1.bias', 'transformer.h.30.mlp.w2.bias', 'transformer.h.31.attn.c_proj.bias', 'transformer.h.31.mlp.c_proj.bias', 'transformer.h.31.mlp.w1.bias', 'transformer.h.31.mlp.w2.bias', 'transformer.h.4.attn.c_proj.bias', 'transformer.h.4.mlp.c_proj.bias', 'transformer.h.4.mlp.w1.bias', 'transformer.h.4.mlp.w2.bias', 'transformer.h.5.attn.c_proj.bias', 'transformer.h.5.mlp.c_proj.bias', 'transformer.h.5.mlp.w1.bias', 'transformer.h.5.mlp.w2.bias', 'transformer.h.6.attn.c_proj.bias', 'transformer.h.6.mlp.c_proj.bias', 'transformer.h.6.mlp.w1.bias', 'transformer.h.6.mlp.w2.bias', 'transformer.h.7.attn.c_proj.bias', 'transformer.h.7.mlp.c_proj.bias', 'transformer.h.7.mlp.w1.bias', 'transformer.h.7.mlp.w2.bias', 'transformer.h.8.attn.c_proj.bias', 'transformer.h.8.mlp.c_proj.bias', 'transformer.h.8.mlp.w1.bias', 'transformer.h.8.mlp.w2.bias', 'transformer.h.9.attn.c_proj.bias', 'transformer.h.9.mlp.c_proj.bias', 'transformer.h.9.mlp.w1.bias', 'transformer.h.9.mlp.w2.bias']\n",
      "- This IS expected if you are initializing QWenLMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing QWenLMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.\n",
      "trainable params: 143,130,624 || all params: 1,388,056,576 || trainable%: 10.3116\n",
      "Loading data...\n",
      "Formatting inputs...Skip in lazy mode\n",
      "/root/miniconda3/lib/python3.10/site-packages/accelerate/accelerator.py:451: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)\n",
      "  warnings.warn(\n",
      "You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.\n",
      "Using /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...\n",
      "Creating extension directory /root/.cache/torch_extensions/py310_cu118/fused_adam...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py310_cu118/fused_adam/build.ninja...\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "[1/3] c++ -MMD -MF fused_adam_frontend.o.d -DTORCH_EXTENSION_NAME=fused_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/root/miniconda3/lib/python3.10/site-packages/deepspeed/ops/csrc/includes -I/root/miniconda3/lib/python3.10/site-packages/deepspeed/ops/csrc/adam -isystem /root/miniconda3/lib/python3.10/site-packages/torch/include -isystem /root/miniconda3/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /root/miniconda3/lib/python3.10/site-packages/torch/include/TH -isystem /root/miniconda3/lib/python3.10/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /root/miniconda3/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -O3 -std=c++17 -g -Wno-reorder -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DBF16_AVAILABLE -c /root/miniconda3/lib/python3.10/site-packages/deepspeed/ops/csrc/adam/fused_adam_frontend.cpp -o fused_adam_frontend.o \n",
      "[2/3] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=fused_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/root/miniconda3/lib/python3.10/site-packages/deepspeed/ops/csrc/includes -I/root/miniconda3/lib/python3.10/site-packages/deepspeed/ops/csrc/adam -isystem /root/miniconda3/lib/python3.10/site-packages/torch/include -isystem /root/miniconda3/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /root/miniconda3/lib/python3.10/site-packages/torch/include/TH -isystem /root/miniconda3/lib/python3.10/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /root/miniconda3/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_89,code=compute_89 -gencode=arch=compute_89,code=sm_89 --compiler-options '-fPIC' -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -lineinfo --use_fast_math -gencode=arch=compute_89,code=sm_89 -gencode=arch=compute_89,code=compute_89 -DBF16_AVAILABLE -U__CUDA_NO_BFLOAT16_OPERATORS__ -U__CUDA_NO_BFLOAT162_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ -std=c++17 -c /root/miniconda3/lib/python3.10/site-packages/deepspeed/ops/csrc/adam/multi_tensor_adam.cu -o multi_tensor_adam.cuda.o \n",
      "[3/3] c++ fused_adam_frontend.o multi_tensor_adam.cuda.o -shared -L/root/miniconda3/lib/python3.10/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o fused_adam.so\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 24.174529552459717 seconds\n",
      "  0%|                                                    | 0/80 [00:00<?, ?it/s]/root/miniconda3/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "{'loss': 0.6435, 'learning_rate': 0.0, 'epoch': 0.06}                           \n",
      "{'loss': 0.5102, 'learning_rate': 0.0003, 'epoch': 0.12}                        \n",
      "{'loss': 0.5319, 'learning_rate': 0.0003, 'epoch': 0.18}                        \n",
      "{'loss': 0.5249, 'learning_rate': 0.0003, 'epoch': 0.24}                        \n",
      "{'loss': 0.5758, 'learning_rate': 0.0003, 'epoch': 0.3}                         \n",
      "{'loss': 0.7782, 'learning_rate': 0.0003, 'epoch': 0.36}                        \n",
      "{'loss': 0.584, 'learning_rate': 0.0003, 'epoch': 0.42}                         \n",
      "{'loss': 0.5913, 'learning_rate': 0.0003, 'epoch': 0.48}                        \n",
      "{'loss': 0.6279, 'learning_rate': 0.0003, 'epoch': 0.54}                        \n",
      "{'loss': 0.6307, 'learning_rate': 0.0003, 'epoch': 0.6}                         \n",
      "{'loss': 0.6295, 'learning_rate': 0.0003, 'epoch': 0.66}                        \n",
      "{'loss': 0.601, 'learning_rate': 0.0003, 'epoch': 0.72}                         \n",
      "{'loss': 0.6328, 'learning_rate': 0.0003, 'epoch': 0.78}                        \n",
      "{'loss': 0.5659, 'learning_rate': 0.0003, 'epoch': 0.84}                        \n",
      "{'loss': 0.6181, 'learning_rate': 0.0003, 'epoch': 0.9}                         \n",
      "{'loss': 0.6732, 'learning_rate': 0.0003, 'epoch': 0.96}                        \n",
      "{'loss': 0.5279, 'learning_rate': 0.0003, 'epoch': 1.01}                        \n",
      "{'loss': 0.5932, 'learning_rate': 0.0003, 'epoch': 1.07}                        \n",
      "{'loss': 0.4406, 'learning_rate': 0.0003, 'epoch': 1.13}                        \n",
      "{'loss': 0.6284, 'learning_rate': 0.0003, 'epoch': 1.19}                        \n",
      "{'loss': 0.4158, 'learning_rate': 0.0003, 'epoch': 1.25}                        \n",
      "{'loss': 0.4846, 'learning_rate': 0.0003, 'epoch': 1.31}                        \n",
      "{'loss': 0.481, 'learning_rate': 0.0003, 'epoch': 1.37}                         \n",
      "{'loss': 0.4533, 'learning_rate': 0.0003, 'epoch': 1.43}                        \n",
      "{'loss': 0.3601, 'learning_rate': 0.0003, 'epoch': 1.49}                        \n",
      "{'loss': 0.485, 'learning_rate': 0.0003, 'epoch': 1.55}                         \n",
      "{'loss': 0.4841, 'learning_rate': 0.0003, 'epoch': 1.61}                        \n",
      "{'loss': 0.3405, 'learning_rate': 0.0003, 'epoch': 1.67}                        \n",
      "{'loss': 0.4511, 'learning_rate': 0.0003, 'epoch': 1.73}                        \n",
      "{'loss': 0.4311, 'learning_rate': 0.0003, 'epoch': 1.79}                        \n",
      "{'loss': 0.3876, 'learning_rate': 0.0003, 'epoch': 1.85}                        \n",
      "{'loss': 0.4155, 'learning_rate': 0.0003, 'epoch': 1.91}                        \n",
      "{'loss': 0.4055, 'learning_rate': 0.0003, 'epoch': 1.97}                        \n",
      "{'loss': 0.3272, 'learning_rate': 0.0003, 'epoch': 2.03}                        \n",
      "{'loss': 0.301, 'learning_rate': 0.0003, 'epoch': 2.09}                         \n",
      "{'loss': 0.2984, 'learning_rate': 0.0003, 'epoch': 2.15}                        \n",
      "{'loss': 0.2612, 'learning_rate': 0.0003, 'epoch': 2.21}                        \n",
      "{'loss': 0.2743, 'learning_rate': 0.0003, 'epoch': 2.27}                        \n",
      "{'loss': 0.2481, 'learning_rate': 0.0003, 'epoch': 2.33}                        \n",
      "{'loss': 0.1565, 'learning_rate': 0.0003, 'epoch': 2.39}                        \n",
      "{'loss': 0.1466, 'learning_rate': 0.0003, 'epoch': 2.45}                        \n",
      "{'loss': 0.2762, 'learning_rate': 0.0003, 'epoch': 2.51}                        \n",
      "{'loss': 0.0886, 'learning_rate': 0.0003, 'epoch': 2.57}                        \n",
      "{'loss': 0.3124, 'learning_rate': 0.0003, 'epoch': 2.63}                        \n",
      "{'loss': 0.3678, 'learning_rate': 0.0003, 'epoch': 2.69}                        \n",
      "{'loss': 0.2266, 'learning_rate': 0.0003, 'epoch': 2.75}                        \n",
      "{'loss': 0.2685, 'learning_rate': 0.0003, 'epoch': 2.81}                        \n",
      "{'loss': 0.2025, 'learning_rate': 0.0003, 'epoch': 2.87}                        \n",
      "{'loss': 0.2911, 'learning_rate': 0.0003, 'epoch': 2.93}                        \n",
      "{'loss': 0.1909, 'learning_rate': 0.0003, 'epoch': 2.99}                        \n",
      "{'loss': 0.1448, 'learning_rate': 0.0003, 'epoch': 3.04}                        \n",
      "{'loss': 0.1311, 'learning_rate': 0.0003, 'epoch': 3.1}                         \n",
      "{'loss': 0.1261, 'learning_rate': 0.0003, 'epoch': 3.16}                        \n",
      "{'loss': 0.1336, 'learning_rate': 0.0003, 'epoch': 3.22}                        \n",
      "{'loss': 0.1228, 'learning_rate': 0.0003, 'epoch': 3.28}                        \n",
      "{'loss': 0.1528, 'learning_rate': 0.0003, 'epoch': 3.34}                        \n",
      "{'loss': 0.0751, 'learning_rate': 0.0003, 'epoch': 3.4}                         \n",
      "{'loss': 0.0901, 'learning_rate': 0.0003, 'epoch': 3.46}                        \n",
      "{'loss': 0.1052, 'learning_rate': 0.0003, 'epoch': 3.52}                        \n",
      "{'loss': 0.0868, 'learning_rate': 0.0003, 'epoch': 3.58}                        \n",
      "{'loss': 0.079, 'learning_rate': 0.0003, 'epoch': 3.64}                         \n",
      "{'loss': 0.0958, 'learning_rate': 0.0003, 'epoch': 3.7}                         \n",
      "{'loss': 0.1816, 'learning_rate': 0.0003, 'epoch': 3.76}                        \n",
      "{'loss': 0.1203, 'learning_rate': 0.0003, 'epoch': 3.82}                        \n",
      "{'loss': 0.1341, 'learning_rate': 0.0003, 'epoch': 3.88}                        \n",
      "{'loss': 0.0534, 'learning_rate': 0.0003, 'epoch': 3.94}                        \n",
      "{'loss': 0.0571, 'learning_rate': 0.0003, 'epoch': 4.0}                         \n",
      "{'loss': 0.0409, 'learning_rate': 0.0003, 'epoch': 4.06}                        \n",
      "{'loss': 0.0328, 'learning_rate': 0.0003, 'epoch': 4.12}                        \n",
      "{'loss': 0.0481, 'learning_rate': 0.0003, 'epoch': 4.18}                        \n",
      "{'loss': 0.0463, 'learning_rate': 0.0003, 'epoch': 4.24}                        \n",
      "{'loss': 0.0605, 'learning_rate': 0.0003, 'epoch': 4.3}                         \n",
      "{'loss': 0.024, 'learning_rate': 0.0003, 'epoch': 4.36}                         \n",
      "{'loss': 0.0536, 'learning_rate': 0.0003, 'epoch': 4.42}                        \n",
      "{'loss': 0.0521, 'learning_rate': 0.0003, 'epoch': 4.48}                        \n",
      "{'loss': 0.0259, 'learning_rate': 0.0003, 'epoch': 4.54}                        \n",
      "{'loss': 0.1122, 'learning_rate': 0.0003, 'epoch': 4.6}                         \n",
      "{'loss': 0.0409, 'learning_rate': 0.0003, 'epoch': 4.66}                        \n",
      "{'loss': 0.0566, 'learning_rate': 0.0003, 'epoch': 4.72}                        \n",
      "{'loss': 0.0336, 'learning_rate': 0.0003, 'epoch': 4.78}                        \n",
      "{'train_runtime': 472.0098, 'train_samples_per_second': 2.828, 'train_steps_per_second': 0.169, 'train_loss': 0.30323936641216276, 'epoch': 4.78}\n",
      "100%|███████████████████████████████████████████| 80/80 [07:52<00:00,  5.90s/it]\n"
     ]
    }
   ],
   "source": [
    "!python finetune.py \\\n",
    "  --model_name_or_path Qwen-7B-Chat-Int4 \\\n",
    "  --data_path Belle_sampled_qwen.json \\\n",
    "  --fp16 True \\\n",
    "  --output_dir output_qwen \\\n",
    "  --num_train_epochs 5 \\\n",
    "  --per_device_train_batch_size 2 \\\n",
    "  --per_device_eval_batch_size 1 \\\n",
    "  --gradient_accumulation_steps 8 \\\n",
    "  --evaluation_strategy \"no\" \\\n",
    "  --save_strategy \"steps\" \\\n",
    "  --save_steps 1000 \\\n",
    "  --save_total_limit 10 \\\n",
    "  --learning_rate 3e-4 \\\n",
    "  --weight_decay 0.1 \\\n",
    "  --adam_beta2 0.95 \\\n",
    "  --warmup_ratio 0.01 \\\n",
    "  --lr_scheduler_type \"cosine\" \\\n",
    "  --logging_steps 1 \\\n",
    "  --report_to \"none\" \\\n",
    "  --model_max_length 512 \\\n",
    "  --lazy_preprocess True \\\n",
    "  --gradient_checkpointing \\\n",
    "  --use_lora \\\n",
    "  --q_lora \\\n",
    "  --deepspeed finetune/ds_config_zero2.json "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3bc084b0-459e-429e-946f-984a2b04ca63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA extension not installed.\n",
      "CUDA extension not installed.\n",
      "Try importing flash-attention for faster inference...\n",
      "Warning: import flash_attn rotary fail, please install FlashAttention rotary to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/rotary\n",
      "Warning: import flash_attn rms_norm fail, please install FlashAttention layer_norm to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/layer_norm\n",
      "Warning: import flash_attn fail, please install FlashAttention to get higher efficiency https://github.com/Dao-AILab/flash-attention\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66572ba20e9f453b91e79bf1452b0a69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at Qwen-7B-Chat-Int4 were not used when initializing QWenLMHeadModel: ['transformer.h.0.attn.c_proj.bias', 'transformer.h.0.mlp.c_proj.bias', 'transformer.h.0.mlp.w1.bias', 'transformer.h.0.mlp.w2.bias', 'transformer.h.1.attn.c_proj.bias', 'transformer.h.1.mlp.c_proj.bias', 'transformer.h.1.mlp.w1.bias', 'transformer.h.1.mlp.w2.bias', 'transformer.h.10.attn.c_proj.bias', 'transformer.h.10.mlp.c_proj.bias', 'transformer.h.10.mlp.w1.bias', 'transformer.h.10.mlp.w2.bias', 'transformer.h.11.attn.c_proj.bias', 'transformer.h.11.mlp.c_proj.bias', 'transformer.h.11.mlp.w1.bias', 'transformer.h.11.mlp.w2.bias', 'transformer.h.12.attn.c_proj.bias', 'transformer.h.12.mlp.c_proj.bias', 'transformer.h.12.mlp.w1.bias', 'transformer.h.12.mlp.w2.bias', 'transformer.h.13.attn.c_proj.bias', 'transformer.h.13.mlp.c_proj.bias', 'transformer.h.13.mlp.w1.bias', 'transformer.h.13.mlp.w2.bias', 'transformer.h.14.attn.c_proj.bias', 'transformer.h.14.mlp.c_proj.bias', 'transformer.h.14.mlp.w1.bias', 'transformer.h.14.mlp.w2.bias', 'transformer.h.15.attn.c_proj.bias', 'transformer.h.15.mlp.c_proj.bias', 'transformer.h.15.mlp.w1.bias', 'transformer.h.15.mlp.w2.bias', 'transformer.h.16.attn.c_proj.bias', 'transformer.h.16.mlp.c_proj.bias', 'transformer.h.16.mlp.w1.bias', 'transformer.h.16.mlp.w2.bias', 'transformer.h.17.attn.c_proj.bias', 'transformer.h.17.mlp.c_proj.bias', 'transformer.h.17.mlp.w1.bias', 'transformer.h.17.mlp.w2.bias', 'transformer.h.18.attn.c_proj.bias', 'transformer.h.18.mlp.c_proj.bias', 'transformer.h.18.mlp.w1.bias', 'transformer.h.18.mlp.w2.bias', 'transformer.h.19.attn.c_proj.bias', 'transformer.h.19.mlp.c_proj.bias', 'transformer.h.19.mlp.w1.bias', 'transformer.h.19.mlp.w2.bias', 'transformer.h.2.attn.c_proj.bias', 'transformer.h.2.mlp.c_proj.bias', 'transformer.h.2.mlp.w1.bias', 'transformer.h.2.mlp.w2.bias', 'transformer.h.20.attn.c_proj.bias', 'transformer.h.20.mlp.c_proj.bias', 'transformer.h.20.mlp.w1.bias', 'transformer.h.20.mlp.w2.bias', 'transformer.h.21.attn.c_proj.bias', 'transformer.h.21.mlp.c_proj.bias', 'transformer.h.21.mlp.w1.bias', 'transformer.h.21.mlp.w2.bias', 'transformer.h.22.attn.c_proj.bias', 'transformer.h.22.mlp.c_proj.bias', 'transformer.h.22.mlp.w1.bias', 'transformer.h.22.mlp.w2.bias', 'transformer.h.23.attn.c_proj.bias', 'transformer.h.23.mlp.c_proj.bias', 'transformer.h.23.mlp.w1.bias', 'transformer.h.23.mlp.w2.bias', 'transformer.h.24.attn.c_proj.bias', 'transformer.h.24.mlp.c_proj.bias', 'transformer.h.24.mlp.w1.bias', 'transformer.h.24.mlp.w2.bias', 'transformer.h.25.attn.c_proj.bias', 'transformer.h.25.mlp.c_proj.bias', 'transformer.h.25.mlp.w1.bias', 'transformer.h.25.mlp.w2.bias', 'transformer.h.26.attn.c_proj.bias', 'transformer.h.26.mlp.c_proj.bias', 'transformer.h.26.mlp.w1.bias', 'transformer.h.26.mlp.w2.bias', 'transformer.h.27.attn.c_proj.bias', 'transformer.h.27.mlp.c_proj.bias', 'transformer.h.27.mlp.w1.bias', 'transformer.h.27.mlp.w2.bias', 'transformer.h.28.attn.c_proj.bias', 'transformer.h.28.mlp.c_proj.bias', 'transformer.h.28.mlp.w1.bias', 'transformer.h.28.mlp.w2.bias', 'transformer.h.29.attn.c_proj.bias', 'transformer.h.29.mlp.c_proj.bias', 'transformer.h.29.mlp.w1.bias', 'transformer.h.29.mlp.w2.bias', 'transformer.h.3.attn.c_proj.bias', 'transformer.h.3.mlp.c_proj.bias', 'transformer.h.3.mlp.w1.bias', 'transformer.h.3.mlp.w2.bias', 'transformer.h.30.attn.c_proj.bias', 'transformer.h.30.mlp.c_proj.bias', 'transformer.h.30.mlp.w1.bias', 'transformer.h.30.mlp.w2.bias', 'transformer.h.31.attn.c_proj.bias', 'transformer.h.31.mlp.c_proj.bias', 'transformer.h.31.mlp.w1.bias', 'transformer.h.31.mlp.w2.bias', 'transformer.h.4.attn.c_proj.bias', 'transformer.h.4.mlp.c_proj.bias', 'transformer.h.4.mlp.w1.bias', 'transformer.h.4.mlp.w2.bias', 'transformer.h.5.attn.c_proj.bias', 'transformer.h.5.mlp.c_proj.bias', 'transformer.h.5.mlp.w1.bias', 'transformer.h.5.mlp.w2.bias', 'transformer.h.6.attn.c_proj.bias', 'transformer.h.6.mlp.c_proj.bias', 'transformer.h.6.mlp.w1.bias', 'transformer.h.6.mlp.w2.bias', 'transformer.h.7.attn.c_proj.bias', 'transformer.h.7.mlp.c_proj.bias', 'transformer.h.7.mlp.w1.bias', 'transformer.h.7.mlp.w2.bias', 'transformer.h.8.attn.c_proj.bias', 'transformer.h.8.mlp.c_proj.bias', 'transformer.h.8.mlp.w1.bias', 'transformer.h.8.mlp.w2.bias', 'transformer.h.9.attn.c_proj.bias', 'transformer.h.9.mlp.c_proj.bias', 'transformer.h.9.mlp.w1.bias', 'transformer.h.9.mlp.w2.bias']\n",
      "- This IS expected if you are initializing QWenLMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing QWenLMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    " \n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\"output_qwen\", device_map=\"auto\", trust_remote_code=True).eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"output_qwen\", trust_remote_code=True)\n",
    "response, history = model.chat(tokenizer, \"你好\", history=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6e11c390-a582-4e35-938f-0af2832342f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'科技的不断进步已经改变了我们的生活方式，给我们带来了更多的便利和舒适。然而，科技的发展也带来了一定的负面影响，如对传统行业产生冲击，改变人们的生产和消费模式等。所以，我们应该在享受科技带来的便利的同时，也要注意保护传统行业的发展和照顾一些可能被科技发展边缘化的人们。'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b056ec84-ef76-4d58-8031-a716ecd79757",
   "metadata": {},
   "outputs": [],
   "source": [
    "response, history = model.chat(tokenizer, \"总结下面这段文本的摘要，随着科技的飞速发展，我们的生活方式也在悄然改变。智能手机、人工智能、物联网等科技产品的出现，为我们的日常生活带来了更多便利和舒适。比如，我们可以通过智能手机随时随地地获取信息，控制家庭设备，同时感受着人工智能为我们带来的智能化之便。但是，科技进步带来的便利也会对生活形成某种影响，比如，冲击传统行业和职业，改变人们的生产和消费模式。\", history=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b72354-4511-4fb2-bd3b-3fab4286367e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
